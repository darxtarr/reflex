# Chronome Adaptive Batching â€” Research Findings (Brief)

**Date**: 2025-10-07  
**Experiment**: Adaptive batching for network transport (threshold + delay tuning)  
**Status**: âœ… COMPLETE (structure found, tradeoff sensitivity discovered)

---

## Key Results

### Model Performance
- **Test RÂ² = 0.475** (avg of threshold=0.398, delay=0.552)
- **Feature correlations**: arrival_rate (+0.707), queue_depth (+0.655-0.706) â­
- **Performance variance**: CoV = 0.583 (58.3%)
- **Model size**: 5.3 KB (two decision trees)

**Verdict**: **REFLEX-VIABLE** â€” learnable structure detected (RÂ² > 0.3)

### Validation Results
- **Flush rate**: -17.8% (reduced overhead) âœ…
- **p95 latency**: +14.5% (increased) âš ï¸
- **p99 latency**: +9.4% (increased)

**Interpretation**: Model learned to reduce flushes but over-optimized, trading too much latency for overhead reduction.

---

## Scientific Insights

### 1. Curvature Spectrum Position

| Domain | Test RÂ² | Variance | Position |
|--------|---------|----------|----------|
| Thread Pool | 0.035 | 0.17% | Flat |
| **Chronome** | **0.475** | **58.3%** | **Moderate** â­ |
| Sensorium | 0.582 | 76.6% | High |

**Chronome sits in the middle** â€” more structure than thread pools, less than sensor sampling.

### 2. Objective Function Sensitivity

The empirical oracle used:
```
J = 1.0Â·p95_latency + 20.0Â·flush_rate
```

**Lesson**: BETA=20 made overhead reduction dominant. The model correctly optimized the objective but the weights may not reflect real-world priorities.

**Next iteration**: Try BETA=5-10 for better latency-overhead balance.

### 3. Multi-Dimensional Output Complexity

Training two separate trees (threshold + delay) worked well:
- Threshold tree: RÂ²=0.398
- Delay tree: RÂ²=0.552 (better prediction)

Delay is more predictable from arrival rate than threshold.

---

## Comparison to Sensorium

| Metric | Sensorium | Chronome |
|--------|-----------|----------|
| Test RÂ² | 0.582 | 0.475 |
| Validation gains | -3.7% (better) | +14.5% (worse) |
| Flush/overhead | -17.5% energy | -17.8% flushes |

**Both learned structure**, but Sensorium's objective was better-tuned.

---

## Artifacts

- `reflex_forge_gen_synthetic_telemetry_chronome.py` â€” Traffic generator
- `reflex_forge_oracle_chronome_empirical.py` â€” Empirical oracle (24 configs)
- `reflex_forge_trainer_chronome.py` â€” Two-output trainer
- `reflex_forge_validate_chronome.py` â€” Validation harness
- `data/models/chronome.reflex` â€” 5.3 KB model
- `data/telemetry/chronome-labeled.csv` â€” 2000 labeled samples

---

## Next Steps

1. **Retune objective**: BETA=5-10 for better latency-overhead balance
2. **Extended validation**: Test on real network traces (not synthetic)
3. **PID comparison**: Compare reflex vs classical PID controller
4. **CHORUS integration**: Deploy to actual Chronome transport layer

---

## Conclusion

Chronome adaptive batching has **moderate learnable structure** (RÂ²=0.475), validating our reflex methodology. The model successfully learned traffic patterns but the objective function needs rebalancing.

This is **valuable negative-ish result** â€” it shows:
1. âœ… Structure exists (RÂ² proves it)
2. âœ… Model learned the objective (reduced flushes)
3. âš ï¸ Objective weights need tuning (over-optimized for overhead)

**Status**: Tablet ğŸœ 1 (Chronome) â†’ âœ… REFLEX-VIABLE (with caveats)

---

ğŸª± Research complete!
